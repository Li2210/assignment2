{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imperial-prescription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top five users with similar interest based on TF-IDF:\n",
      "[(1012137970885906432, 0.43845261466356117), (282643059, 0.4010760488191615), (138309569, 0.39620107225594015), (545185861, 0.38650057152931333), (848329022211620864, 0.3659575900891094)]\n",
      "Top five users with similar interest based on Word2Vec:\n",
      "[(1323090359581200385, 0.9086233797180083), (875668452463259648, 0.8880533595103648), (1263371505733218304, 0.8880533595103648), (581812461, 0.8880533595103648), (1166466828, 0.8711821362101051)]\n",
      "User recomendation is:\n",
      "17375281 : 24259259   111404710   14216123   221118045   83036348   \n",
      "56722736 : 346102488   1244013067790364673   24259259   3094649957   807095   \n",
      "866949673461981184 : 2374745089   907438896702570496   831772091426930688   143104075   3240396234   \n",
      "776625931234390017 : 346102488   1244013067790364673   24259259   3094649957   807095   \n",
      "1220079661201399809 : 380648579   100248020   32828872   1656958189   1278694316752490500   \n",
      "107548394 : 15859912   26574283   17674244   1891266440   185025785   \n",
      "14849092 : 15859912   26574283   17674244   1891266440   185025785   \n",
      "319372040 : 15859912   26574283   17674244   1891266440   185025785   \n",
      "325856719 : 15115280   20457806   56488059   224329419   14755475   \n",
      "83105742 : 83440337   260371201   17674244   94482117   860654500431880192   \n",
      "1241184520348590080 : 15859912   26574283   17674244   1891266440   185025785   \n",
      "1478659146 : 15859912   26574283   17674244   1891266440   185025785   \n",
      "4474623749 : 133081348   408793153   1267708379138535424   17674244   254999238   \n",
      "116083254 : 15859912   26574283   17674244   1891266440   185025785   \n",
      "3252147534 : 133081348   408793153   1267708379138535424   17674244   254999238   \n",
      "2358121866 : 24259259   346102488   3094649957   1244013067790364673   21802625   \n",
      "864057771523809282 : 380648579   100248020   32828872   1656958189   1278694316752490500   \n",
      "1591286180 : 56488059   20457806   9814812   279595834   859850228   \n",
      "14997273 : 346102488   1244013067790364673   24259259   3094649957   807095   \n",
      "3300787924 : 346102488   1244013067790364673   24259259   3094649957   807095   \n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import *\n",
    "\n",
    "# https://stackoverflow.com/questions/57451719/since-spark-2-3-the-queries-from-raw-json-csv-files-are-disallowed-when-the-ref\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "FILE_NAME = \"tweets.json\"\n",
    "\n",
    "original_data = spark.read.option(\"multiline\",\"true\").json(FILE_NAME).cache()\n",
    "\n",
    "selectId = \"480875170\"\n",
    "\n",
    "# Q1\n",
    "\n",
    "# find all replies\n",
    "user_replyTo = original_data.select(\"user_id\", \"replyto_id\").where(\"replyto_id is not null\")\n",
    "# find all retweets\n",
    "user_retweet = original_data.select(\"user_id\", \"retweet_id\").where(\"retweet_id is not null\")\n",
    "\n",
    "# combine two data:(user_id, relatedTweetId)\n",
    "user_combine = user_retweet.union(user_replyTo)\n",
    "user_data = user_combine.rdd.map(lambda row:(row[0], str(row[1]))).groupByKey().mapValues(list)\n",
    "\n",
    "def changeToString(row):\n",
    "    user_id = row[0]\n",
    "    data = \" \".join(row[1])\n",
    "    return (user_id, data)\n",
    "    \n",
    "    \n",
    "user_tweet_string = user_data.map(changeToString)\n",
    "\n",
    "train_data = user_tweet_string.toDF([\"user_id\", \"data\"])\n",
    "\n",
    "# idf train data\n",
    "# https://spark.apache.org/docs/latest/ml-features.html#tf-idf\n",
    "tokenizer = Tokenizer(inputCol=\"data\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(train_data)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\",numFeatures=131072)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# drop useless columns\n",
    "clearData = rescaledData.drop(\"data\").drop(\"words\").drop(\"rawFeatures\")\n",
    "\n",
    "target_user = clearData.filter(\"user_id =\" + selectId).collect()[0][1]\n",
    "other_user = clearData.filter(\"user_id !=\" + selectId).rdd.map(lambda row : (row[0], row[1]))\n",
    "\n",
    "def cosine(a,b):\n",
    "    return a.dot(b) / (a.norm(2) * b.norm(2))\n",
    "\n",
    "found_user = other_user.map(lambda row:(row[0], cosine(target_user, row[1])))\n",
    "idf_result = found_user.sortBy(lambda row:row[1], ascending=False).take(5)\n",
    "\n",
    "\n",
    "\n",
    "# Word2Vec\n",
    "text_data = user_tweet_string.toDF([\"user_id\", \"text\"])\n",
    "word_data = text_data.select('user_id', split(\"text\",' ').alias(\"words\"))\n",
    "# word_data.show(100, truncate=False)\n",
    "\n",
    "# train data\n",
    "word2Vec = Word2Vec(vectorSize=300, minCount=1, inputCol=\"words\", outputCol=\"vector\")\n",
    "model = word2Vec.fit(word_data)\n",
    "result = model.transform(word_data).drop(\"words\")\n",
    "\n",
    "target_user_1 = result.filter(\"user_id =\" + selectId).collect()[0][1]\n",
    "other_user_1 = result.filter(\"user_id !=\" + selectId).rdd.map(lambda row : (row[0], row[1]))\n",
    "found_user_1 = other_user_1.map(lambda row:(row[0], cosine(target_user_1, row[1])))\n",
    "word_result = found_user_1.sortBy(lambda row:row[1], ascending=False).take(5)\n",
    "\n",
    "\n",
    "# get user_id \n",
    "user_ids = original_data.select(\"user_id\").rdd.map(lambda row:row[0]).distinct().collect()\n",
    "\n",
    "# create dictionary for user_id\n",
    "user_id_mapper = dict()\n",
    "for user_id in user_ids:\n",
    "    user_id_mapper[user_id] = len(user_id_mapper)\n",
    "\n",
    "def extract_id(rows):\n",
    "    return [(mention_user[0]) for mention_user in rows[0]]\n",
    "\n",
    "# extract mentioned user id\n",
    "mention_user_info = original_data.select(\"user_mentions\").where(\"user_mentions is not null\")\n",
    "mention_user_ids = mention_user_info.rdd.flatMap(extract_id).distinct().collect()\n",
    "# create dictionary for mentioned user id\n",
    "mention_user_id_mapper = dict()\n",
    "for mention_user_id in mention_user_ids:\n",
    "    mention_user_id_mapper[mention_user_id] = len(mention_user_id_mapper)\n",
    "\n",
    "# sc.broadcast(user_id_mapper)\n",
    "# sc.broadcast(mention_user_id_mapper)\n",
    "\n",
    "def extract_data(rows):\n",
    "    user_id = rows[0]\n",
    "    return [(user_id, mention_user_id_mapper[mention_user[0]], 1) for mention_user in rows[1]]\n",
    "\n",
    "def create_key(rows):\n",
    "    user_id = rows[0] \n",
    "    mention_user = rows[1] \n",
    "    counter = rows[2]\n",
    "    return ((user_id,mention_user), counter)\n",
    "\n",
    "mention_user = original_data.select(\"user_id\",\"user_mentions\").where(\"user_mentions is not null\")\n",
    "# extract data\n",
    "clear_data = mention_user.rdd.map(lambda row:(user_id_mapper[row[0]],row[1])).flatMap(extract_data)\n",
    "# combine user_id mention_user_id as a key\n",
    "key_data = clear_data.map(create_key)\n",
    "# calculate                    \n",
    "clear_key = key_data.reduceByKey(lambda a,b : a + b).map(lambda row : (row[0][0], row[0][1], row[1]))\n",
    "train_data = clear_key.toDF([\"user_id\", \"mention_user_id\", \"counter\"])\n",
    "\n",
    "# https://spark.apache.org/docs/latest/ml-collaborative-filtering.html\n",
    "# train data\n",
    "als = ALS(userCol=\"user_id\", itemCol=\"mention_user_id\", ratingCol=\"counter\",coldStartStrategy=\"drop\")\n",
    "model = als.fit(train_data)\n",
    "model_recommend = model.recommendForAllUsers(5).collect()\n",
    "\n",
    "print(\"Top five users with similar interest based on TF-IDF:\")\n",
    "print(idf_result)\n",
    "\n",
    "print(\"Top five users with similar interest based on Word2Vec:\")\n",
    "print(word_result)\n",
    "\n",
    "print(\"User recomendation is:\")\n",
    "counter = 0\n",
    "\n",
    "for rows in model_recommend:\n",
    "    if(counter < 20):\n",
    "        print(user_ids[rows[0]], \":\", end=\" \")\n",
    "        for data in rows[1]:\n",
    "            print(mention_user_ids[data[0]], end=\"   \")\n",
    "        print()\n",
    "        counter += 1\n",
    "\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-queensland",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
